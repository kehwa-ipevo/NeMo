{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[NeMo W 2024-10-28 22:56:08 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import nemo_run as run\n",
    "from typing import Optional\n",
    "import pytorch_lightning as pl\n",
    "from nemo.collections import llm\n",
    "from nemo.collections.common.tokenizers import SentencePieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slimpajama(\n",
    "    gbs: int = 256,\n",
    "    mbs: int = 4,\n",
    "    seq_length: int = 8192,\n",
    ") -> run.Config[pl.LightningDataModule]:\n",
    "\n",
    "    return run.Config(\n",
    "        llm.PreTrainingDataModule,\n",
    "        paths=[\"/data/slimpajama_megatron/concatenated_chunk1.jsonl_text_document\"],\n",
    "        seq_length=seq_length,\n",
    "        global_batch_size=gbs,\n",
    "        micro_batch_size=mbs,\n",
    "        tokenizer=run.Config(SentencePieceTokenizer, model_path=\"/data/tokenizer/tokenizer.model\"),\n",
    "        split=\"99990,8,2\",\n",
    "        num_workers=2,\n",
    "        index_mapping_dir=\"/data/index_mapping\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_recipe(nodes: int = 1, gpus_per_node: int = 1):\n",
    "    recipe = llm.llama3_8b.pretrain_recipe(\n",
    "        dir=\"/checkpoints/llama-new\", # Path to store checkpoints\n",
    "        name=\"llama_pretraining\",\n",
    "        num_nodes=nodes,\n",
    "        num_gpus_per_node=gpus_per_node,\n",
    "    )\n",
    "\n",
    "    recipe.model.config.num_layers = 1\n",
    "    recipe.model.config.hidden_size = 128\n",
    "    recipe.trainer.max_steps = 30\n",
    "    recipe.data = slimpajama(\n",
    "        gbs=32,\n",
    "        mbs=1,\n",
    "    )\n",
    "    recipe.trainer.val_check_interval = 20\n",
    "    recipe.trainer.strategy.context_parallel_size = 1\n",
    "    recipe.log.ckpt.save_optim_on_train_end = True\n",
    "    return recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_executor_torchrun(nodes: int = 1, devices: int = 1) -> run.LocalExecutor:\n",
    "    # Env vars for jobs are configured here\n",
    "    env_vars = {\n",
    "        \"TORCH_NCCL_AVOID_RECORD_STREAMS\": \"1\",\n",
    "        \"NEMO_ENV_VARNAME_TESTING\": \"1\",\n",
    "        \"CUDA_VISIBLE_DEVICES\": \"0\"\n",
    "    }\n",
    "\n",
    "    executor = run.LocalExecutor(ntasks_per_node=devices, launcher=\"torchrun\", env_vars=env_vars)\n",
    "    return executor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pretraining():\n",
    "    recipe = configure_recipe()\n",
    "    executor = local_executor_torchrun(nodes=recipe.trainer.num_nodes, devices=recipe.trainer.devices)\n",
    "\n",
    "    run.run(recipe, executor=executor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Entering Experiment nemo.collections.llm.api.pretrain with id: nemo.collections.llm.api.pretrain_1730156175</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ───</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─── \u001b[0m\u001b[1;35mEntering Experiment nemo.collections.llm.api.pretrain with id: nemo.collections.llm.api.pretrain_1730156175\u001b[0m\u001b[92m ───\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/nemo.collections.llm.api.pretrain/nemo.collections.llm.api.pretrain_1730156175/nemo.collections.llm.api.pretrain\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[22:56:15] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Launching job nemo.collections.llm.api.pretrain for experiment </span>                        <a href=\"file:///opt/NeMo-Run/src/nemo_run/run/experiment.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">experiment.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/NeMo-Run/src/nemo_run/run/experiment.py#660\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">660</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">nemo.collections.llm.api.pretrain</span>                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[22:56:15]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;36mLaunching job nemo.collections.llm.api.pretrain for experiment \u001b[0m                        \u001b]8;id=621195;file:///opt/NeMo-Run/src/nemo_run/run/experiment.py\u001b\\\u001b[2mexperiment.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=194979;file:///opt/NeMo-Run/src/nemo_run/run/experiment.py#660\u001b\\\u001b[2m660\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;36mnemo.collections.llm.api.pretrain\u001b[0m                                                      \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /root/.nemo_run/experiments/nemo.collections.llm.api.pretrain/nemo.collections.llm.api.pretrain_1730156175/nemo.collections.llm.api.pretrain\n",
      "Launched app: local_persistent://nemo_run/nemo.collections.llm.api.pretrain-s0ccr7w39dv59c\n",
      "AppStatus:\n",
      "    State: RUNNING\n",
      "    Num Restarts: 0\n",
      "    Roles: \n",
      "    Msg: <NONE>\n",
      "    Structured Error Msg: <NONE>\n",
      "    UI URL: file:///root/.nemo_run/experiments/nemo.collections.llm.api.pretrain/nemo.collections.llm.api.pretrain_1730156175/nemo.collections.llm.api.pretrain/nemo_run/nemo.collections.llm.api.pretrain-s0ccr7w39dv59c\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">────────────────── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Waiting for Experiment nemo.collections.llm.api.pretrain_1730156175 to finish</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ──────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m────────────────── \u001b[0m\u001b[1;35mWaiting for Experiment nemo.collections.llm.api.pretrain_1730156175 to finish\u001b[0m\u001b[92m ──────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Experiment Status for</span> <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.pretrain_1730156175</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mExperiment Status for\u001b[0m \u001b[1;38;5;214mnemo.collections.llm.api.pretrain_1730156175\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Task 0</span>: <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.pretrain</span>\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Status</span>: RUNNING\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Executor</span>: LocalExecutor\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Job id</span>: nemo.collections.llm.api.pretrain-s0ccr7w39dv59c\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Local Directory</span>: /root/.nemo_run/experiments/nemo.collections.llm.api.pretrain/nemo.collections.llm.api.pretrain_1730156175/nemo.collections.llm.api.pretrain\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mTask 0\u001b[0m: \u001b[1;38;5;214mnemo.collections.llm.api.pretrain\u001b[0m\n",
       "- \u001b[1;32mStatus\u001b[0m: RUNNING\n",
       "- \u001b[1;32mExecutor\u001b[0m: LocalExecutor\n",
       "- \u001b[1;32mJob id\u001b[0m: nemo.collections.llm.api.pretrain-s0ccr7w39dv59c\n",
       "- \u001b[1;32mLocal Directory\u001b[0m: /root/.nemo_run/experiments/nemo.collections.llm.api.pretrain/nemo.collections.llm.api.pretrain_1730156175/nemo.collections.llm.api.pretrain\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for job nemo.collections.llm.api.pretrain-s0ccr7w39dv59c to finish [log=True]...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i.pretrain/0 I1028 22:56:16.514000 140701987661632 torch/distributed/launcher/api.py:188] Starting elastic_operator with launch configs:\n",
      "i.pretrain/0 I1028 22:56:16.514000 140701987661632 torch/distributed/launcher/api.py:188]   entrypoint       : nemo_run.core.runners.fdl_runner\n",
      "i.pretrain/0 I1028 22:56:16.514000 140701987661632 torch/distributed/launcher/api.py:188]   min_nodes        : 1\n",
      "i.pretrain/0 I1028 22:56:16.514000 140701987661632 torch/distributed/launcher/api.py:188]   max_nodes        : 1\n",
      "i.pretrain/0 I1028 22:56:16.514000 140701987661632 torch/distributed/launcher/api.py:188]   nproc_per_node   : 1\n",
      "i.pretrain/0 I1028 22:56:16.514000 140701987661632 torch/distributed/launcher/api.py:188]   run_id           : 8678\n",
      "i.pretrain/0 I1028 22:56:16.514000 140701987661632 torch/distributed/launcher/api.py:188]   rdzv_backend     : c10d\n",
      "i.pretrain/0 I1028 22:56:16.514000 140701987661632 torch/distributed/launcher/api.py:188]   rdzv_endpoint    : localhost:0\n",
      "i.pretrain/0 I1028 22:56:16.514000 140701987661632 torch/distributed/launcher/api.py:188]   entrypoint       : nemo_run.core.runners.fdl_runner\n",
      "i.pretrain/0 I1028 22:56:16.514000 140701987661632 torch/distributed/launcher/api.py:188]   min_nodes        : 1\n",
      "i.pretrain/0 I1028 22:56:16.514000 140701987661632 torch/distributed/launcher/api.py:188]   max_nodes        : 1\n",
      "i.pretrain/0 I1028 22:56:16.514000 140701987661632 torch/distributed/launcher/api.py:188]   nproc_per_node   : 1\n",
      "i.pretrain/0 I1028 22:56:16.514000 140701987661632 torch/distributed/launcher/api.py:188]   run_id           : 8678\n",
      "i.pretrain/0 I1028 22:56:16.514000 140701987661632 torch/distributed/launcher/api.py:188]   rdzv_backend     : c10d\n",
      "i.pretrain/0 I1028 22:56:16.514000 140701987661632 torch/distributed/launcher/api.py:188]   rdzv_endpoint    : localhost:0\n",
      "i.pretrain/0 I1028 22:56:16.514000 140701987661632 torch/distributed/launcher/api.py:188]   rdzv_configs     : {'timeout': 900}\n",
      "i.pretrain/0 I1028 22:56:16.514000 140701987661632 torch/distributed/launcher/api.py:188]   max_restarts     : 0\n",
      "i.pretrain/0 I1028 22:56:16.514000 140701987661632 torch/distributed/launcher/api.py:188]   monitor_interval : 0.1\n",
      "i.pretrain/0 I1028 22:56:16.514000 140701987661632 torch/distributed/launcher/api.py:188]   log_dir          : /root/.nemo_run/experiments/nemo.collections.llm.api.pretrain/nemo.collections.llm.api.pretrain_1730156175/nemo.collections.llm.api.pretrain/nemo_run/nemo.collections.llm.api.pretrain-s0ccr7w39dv59c/torchelastic/nemo.collections.llm.api.pretrain\n",
      "i.pretrain/0 I1028 22:56:16.514000 140701987661632 torch/distributed/launcher/api.py:188]   metrics_cfg      : {}\n",
      "i.pretrain/0 I1028 22:56:16.514000 140701987661632 torch/distributed/launcher/api.py:188] \n",
      "i.pretrain/0 I1028 22:56:16.516000 140701987661632 torch/distributed/elastic/agent/server/api.py:825] [default] starting workers for entrypoint: python\n",
      "i.pretrain/0 I1028 22:56:16.516000 140701987661632 torch/distributed/elastic/agent/server/api.py:646] [default] Rendezvous'ing worker group\n",
      "i.pretrain/0 I1028 22:56:16.743000 140701987661632 torch/distributed/elastic/agent/server/api.py:512] [default] Rendezvous complete for workers. Result:\n",
      "i.pretrain/0 I1028 22:56:16.743000 140701987661632 torch/distributed/elastic/agent/server/api.py:512]   restart_count=0\n",
      "i.pretrain/0 I1028 22:56:16.743000 140701987661632 torch/distributed/elastic/agent/server/api.py:512]   master_addr=72ea2f280911\n",
      "i.pretrain/0 I1028 22:56:16.743000 140701987661632 torch/distributed/elastic/agent/server/api.py:512]   master_port=36891\n",
      "i.pretrain/0 I1028 22:56:16.743000 140701987661632 torch/distributed/elastic/agent/server/api.py:512]   group_rank=0\n",
      "i.pretrain/0 I1028 22:56:16.743000 140701987661632 torch/distributed/elastic/agent/server/api.py:512]   group_world_size=1\n",
      "i.pretrain/0 I1028 22:56:16.743000 140701987661632 torch/distributed/elastic/agent/server/api.py:512]   local_ranks=[0]\n",
      "i.pretrain/0 I1028 22:56:16.743000 140701987661632 torch/distributed/elastic/agent/server/api.py:512]   role_ranks=[0]\n",
      "i.pretrain/0 I1028 22:56:16.743000 140701987661632 torch/distributed/elastic/agent/server/api.py:512]   global_ranks=[0]\n",
      "i.pretrain/0 I1028 22:56:16.743000 140701987661632 torch/distributed/elastic/agent/server/api.py:512]   role_world_sizes=[1]\n",
      "i.pretrain/0 I1028 22:56:16.743000 140701987661632 torch/distributed/elastic/agent/server/api.py:512]   global_world_sizes=[1]\n",
      "i.pretrain/0 I1028 22:56:16.743000 140701987661632 torch/distributed/elastic/agent/server/api.py:512] \n",
      "i.pretrain/0 I1028 22:56:16.744000 140701987661632 torch/distributed/elastic/agent/server/api.py:654] [default] Starting worker group\n",
      "i.pretrain/0 I1028 22:56:16.744000 140701987661632 torch/distributed/elastic/agent/server/local_elastic_agent.py:184] Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.\n",
      "i.pretrain/0 I1028 22:56:16.744000 140701987661632 torch/distributed/elastic/agent/server/local_elastic_agent.py:216] Environment variable 'TORCHELASTIC_HEALTH_CHECK_PORT' not found. Do not start health check.\n",
      "i.pretrain/0 [default0]:[NeMo W 2024-10-28 22:56:25 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "i.pretrain/0 [default0]:      cm = get_cmap(\"Set1\")\n",
      "i.pretrain/0 [default0]:    \n",
      "i.pretrain/0 [default0]:GPU available: True (cuda), used: True\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 nemo_logger:145] Experiments will be logged at /checkpoints/llama-new/llama_pretraining\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 megatron_strategy:291] Fixing mis-match between ddp-config & mcore-optimizer config\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 megatron_init:314] Rank 0 has data parallel group : [0]\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 megatron_init:320] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 megatron_init:325] All data parallel group ranks with context parallel combined: [[0]]\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 megatron_init:328] Ranks 0 has data parallel rank: 0\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 megatron_init:336] Rank 0 has context parallel group: [0]\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 megatron_init:339] All context parallel group ranks: [[0]]\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 megatron_init:340] Ranks 0 has context parallel rank: 0\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 megatron_init:347] Rank 0 has model parallel group: [0]\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 megatron_init:348] All model parallel group ranks: [[0]]\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 megatron_init:357] Rank 0 has tensor model parallel group: [0]\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 megatron_init:361] All tensor model parallel group ranks: [[0]]\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 megatron_init:362] Rank 0 has tensor model parallel rank: 0\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 megatron_init:382] Rank 0 has pipeline model parallel group: [0]\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 megatron_init:394] Rank 0 has embedding group: [0]\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 megatron_init:400] All pipeline model parallel group ranks: [[0]]\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 megatron_init:401] Rank 0 has pipeline model parallel rank 0\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 megatron_init:402] All embedding group ranks: [[0]]\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 megatron_init:403] Rank 0 has embedding rank: 0\n",
      "i.pretrain/0 [default0]:TPU available: False, using: 0 TPU cores\n",
      "i.pretrain/0 [default0]:HPU available: False, using: 0 HPUs\n",
      "i.pretrain/0 [default0]:[NeMo W 2024-10-28 22:56:26 nemo_logger:123] No version folders would be created under the log folder as 'resume_if_exists' is enabled.\n",
      "i.pretrain/0 [default0]:[NeMo W 2024-10-28 22:56:26 nemo_logger:173] \"update_logger_directory\" is True. Overwriting tensorboard logger \"save_dir\" to /checkpoints/llama-new/tb_logs\n",
      "i.pretrain/0 [default0]:[NeMo W 2024-10-28 22:56:26 nemo_logger:189] The Trainer already contains a ModelCheckpoint callback. This will be overwritten.\n",
      "i.pretrain/0 [default0]:[NeMo W 2024-10-28 22:56:26 nemo_logger:212] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 30. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "i.pretrain/0 [default0]:[NeMo W 2024-10-28 22:56:26 resume:215] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/checkpoints/llama-new/llama_pretraining/checkpoints. Training from scratch.\n",
      "i.pretrain/0 [default0]:Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "i.pretrain/0 [default0]:----------------------------------------------------------------------------------------------------\n",
      "i.pretrain/0 [default0]:distributed_backend=nccl\n",
      "i.pretrain/0 [default0]:All distributed processes registered. Starting with 1 processes\n",
      "i.pretrain/0 [default0]:----------------------------------------------------------------------------------------------------\n",
      "i.pretrain/0 [default0]:\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 utils:259] Let split_matrix = [(0, 0.9999), (0.9999, 0.99998), (0.99998, 1.0)]\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 utils:259] Building dataset splits with cls=GPTDataset, sizes=[960, 2048, 1600], and config=GPTDatasetConfig(random_seed=1234, sequence_length=8192, blend=[['/data/slimpajama_megatron/concatenated_chunk1.jsonl_text_document'], None], blend_per_split=None, renormalize_blend_weights=False, split='99990,8,2', split_matrix=[(0, 0.9999), (0.9999, 0.99998), (0.99998, 1.0)], num_dataset_builder_threads=1, path_to_cache='/data/index_mapping', mmap_bin_files=True, mock=False, tokenizer=<nemo.collections.common.tokenizers.sentencepiece_tokenizer.SentencePieceTokenizer object at 0x79f59d640910>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask=False, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, s3_cache_path=None)\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 utils:259] Load the _IndexReader from /data/slimpajama_megatron/concatenated_chunk1.jsonl_text_document.idx\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 utils:259] \tExtract the sequence lengths\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 utils:259] \tExtract the sequence pointers\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 utils:259] \tExtract the document indices\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 utils:259] > total number of sequences: 109741\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 utils:259] > total number of documents: 109741\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 utils:259] Load the GPTDataset train indices\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 utils:259] \tLoad the document index from 03b17f2210fc6a1e053b6a90f451f0a3-GPTDataset-train-document_index.npy\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 utils:259] \tLoad the sample index from 03b17f2210fc6a1e053b6a90f451f0a3-GPTDataset-train-sample_index.npy\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 utils:259] \tLoad the shuffle index from 03b17f2210fc6a1e053b6a90f451f0a3-GPTDataset-train-shuffle_index.npy\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 utils:259] > total number of samples: 15076\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 utils:259] Load the GPTDataset valid indices\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 utils:259] \tLoad the document index from a09838b0357e1076c2045cdd35bce436-GPTDataset-valid-document_index.npy\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 utils:259] \tLoad the sample index from a09838b0357e1076c2045cdd35bce436-GPTDataset-valid-sample_index.npy\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 utils:259] \tLoad the shuffle index from a09838b0357e1076c2045cdd35bce436-GPTDataset-valid-shuffle_index.npy\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 utils:259] > total number of samples: 2048\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 utils:259] Load the GPTDataset test indices\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 utils:259] \tLoad the document index from b883b79e4094c23eb6767a869a6ffc7b-GPTDataset-test-document_index.npy\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 utils:259] \tLoad the sample index from b883b79e4094c23eb6767a869a6ffc7b-GPTDataset-test-sample_index.npy\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 utils:259] \tLoad the shuffle index from b883b79e4094c23eb6767a869a6ffc7b-GPTDataset-test-shuffle_index.npy\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 utils:259] > total number of samples: 1600\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:26 base:44] Padded vocab_size: 32000, original vocab_size: 32000, dummy tokens: 0.\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:27 num_microbatches_calculator:218] setting number of microbatches to constant 32\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:27 megatron_parallel:549]  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 13738368\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:27 utils:259] Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=True, overlap_param_gather=True, align_param_gather=False, use_distributed_optimizer=True, check_for_nan_in_grad=True, bucket_size=40000000, average_in_collective=True, fp8_param_gather=False)\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:27 utils:280] Number of buckets for gradient all-reduce / reduce-scatter: 1\n",
      "i.pretrain/0 [default0]:    Params for bucket 1 (13738368 elements):\n",
      "i.pretrain/0 [default0]:    \tmodule.decoder.final_layernorm.weight\n",
      "i.pretrain/0 [default0]:    \tmodule.decoder.layers.0.mlp.linear_fc1.layer_norm_weight\n",
      "i.pretrain/0 [default0]:    \tmodule.decoder.layers.0.self_attention.linear_qkv.weight\n",
      "i.pretrain/0 [default0]:    \tmodule.output_layer.weight\n",
      "i.pretrain/0 [default0]:    \tmodule.decoder.layers.0.mlp.linear_fc2.weight\n",
      "i.pretrain/0 [default0]:    \tmodule.decoder.layers.0.mlp.linear_fc1.weight\n",
      "i.pretrain/0 [default0]:    \tmodule.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight\n",
      "i.pretrain/0 [default0]:    \tmodule.decoder.layers.0.self_attention.linear_proj.weight\n",
      "i.pretrain/0 [default0]:    \tmodule.embedding.word_embeddings.weight\n",
      "i.pretrain/0 [default0]:[NeMo I 2024-10-28 22:56:27 utils:259] Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.0003, min_lr=None, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=False, bf16=True, params_dtype=torch.bfloat16, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-05, sgd_momentum=0.9, use_distributed_optimizer=True, overlap_param_gather_with_optimizer_step=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=False, timers=None, config_logger_dir='')\n",
      "i.pretrain/0 [default0]:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "i.pretrain/0 [default0]:\n",
      "i.pretrain/0 [default0]:  | Name   | Type | Params | Mode \n",
      "i.pretrain/0 [default0]:----------------------------------------\n",
      "i.pretrain/0 [default0]:0 | module | DDP  | 13.7 M | train\n",
      "i.pretrain/0 [default0]:----------------------------------------\n",
      "i.pretrain/0 [default0]:13.7 M    Trainable params\n",
      "i.pretrain/0 [default0]:0         Non-trainable params\n",
      "i.pretrain/0 [default0]:13.7 M    Total params\n",
      "i.pretrain/0 [default0]:54.953    Total estimated model params size (MB)\n",
      "i.pretrain/0 [default0]:31        Modules in train mode\n",
      "i.pretrain/0 [default0]:0         Modules in eval mode\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 1/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 2/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 3/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 4/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 5/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 6/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 7/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 8/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 9/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 10/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 11/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 12/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 13/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 14/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 15/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 16/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 17/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 18/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 19/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 20/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 21/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 22/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 23/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 24/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 25/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 26/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 27/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 28/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 29/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 30/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 31/32\n",
      "i.pretrain/0 [default0]:Sanity checking Validation: iteration 32/32\n",
      "i.pretrain/0 [default0]:[NeMo W 2024-10-28 22:57:07 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('global_batch_size', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "i.pretrain/0 [default0]:    \n",
      "i.pretrain/0 [default0]:[NeMo W 2024-10-28 22:57:07 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "i.pretrain/0 [default0]:    \n",
      "i.pretrain/0 [default0]:Training epoch 0, iteration 0/29 | lr: 1.499e-07 | global_batch_size: 32 | global_step: 0 | reduced_train_loss: 10.38 | train_step_timing in s: 10.19\n",
      "i.pretrain/0 [default0]:> /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py(421)_should_check_val_fx()\n",
      "i.pretrain/0 [default0]:-> if not self._should_check_val_epoch():\n",
      "i.pretrain/0 [default0]:(Pdb) "
     ]
    }
   ],
   "source": [
    "run_pretraining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
